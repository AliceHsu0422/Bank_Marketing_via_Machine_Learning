# -*- coding: utf-8 -*-
"""深度學習.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1seKLAKV4tF5bSs7sLg5sGxl2lcKvbOkJ
"""

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold, StratifiedShuffleSplit
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler
import random
import sklearn.metrics as sk
# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import f1_score,accuracy_score,recall_score,auc, roc_auc_score, roc_curve,confusion_matrix,classification_report,precision_score
df = pd.read_csv("bank-additional-full.csv",sep = ';')
print(df.shape)
df.head()

print(df.isnull().sum())#是否有缺值
print(df.info())

plt.title("day of week",fontweight='bold')
sns.countplot(df['day_of_week'],hue=df['y'],order = df['day_of_week'].value_counts().index)

plt.title("month",fontweight='bold')
sns.countplot(df['month'],hue=df['y'],order = df['month'].value_counts().index)

plt.title("contact",fontweight='bold')
sns.countplot(df['contact'],hue=df['y'],order = df['contact'].value_counts().index)

plt.title("poutcome",fontweight='bold')
sns.countplot(df['poutcome'],hue=df['y'],order = df['poutcome'].value_counts().index)

df.drop(columns=['day_of_week','contact'],axis=1,inplace=True)#remove useless columns
df.y.replace(('yes', 'no'), (1, 0), inplace=True)
df.default.replace(('yes', 'no'), (1, 0), inplace=True)
df.housing.replace(('yes', 'no'), (1, 0), inplace=True)
df.loan.replace(('yes', 'no'), (1, 0), inplace=True)
#deal with month
df.head()

df2 = pd.get_dummies(df)
df2.drop(columns=['job_unknown','marital_divorced','education_unknown',"month_may"],axis=1,inplace=True)
df2.head()

df2.corr()["y"].sort_values(ascending = False)

"""# split data and train model"""

print(df2.shape)
df2.head()

df_target=df2[['y']].values
df_features=df2.drop(columns=['y'],axis=1).values
X_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size = 0.25, random_state = 0)
sc = StandardScaler()
x1_train = sc.fit_transform(X_train)
x1_test = sc.transform(X_test)
print(x1_train.shape)
print(y_train.shape)

"""
from imblearn.over_sampling import SMOTE
sm = SMOTE()
x1_train, y_train = sm.fit_resample(x1_train, y_train)

print(x1_train.shape)
print(y_train.shape)"""

def confusionmat(y,y_hat):
    from sklearn.metrics import confusion_matrix,accuracy_score
    cm = confusion_matrix(y, y_hat)
    accu=accuracy_score(y,y_hat)
    print(cm,"\n")
    print("The accuracy is",accu)

#Accuracy and Loss Curves
def learningcurve(history):
    # list all data in history
    print(history.history.keys())
    # summarize history for accuracy
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(52,activation="softmax"))

# Adding the second hidden layer
classifier.add(Dense(35,activation="softmax"))
classifier.add(Dense(20,activation="softmax"))
classifier.add(Dense(10,activation="softmax"))
# Adding the output layer
classifier.add(Dense(1,activation="sigmoid"))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
time_start = time.time() #開始計時

history=classifier.fit(x1_train, y_train, batch_size = 150, epochs=40,validation_split=0.3)
# Predicting the Test set results
y_pred = classifier.predict_classes(x1_test)
pre_score = sk.average_precision_score(y_test, y_pred)
classifier.summary()
test_results = classifier.evaluate(x1_test, y_test)
confusionmat(y_test,y_pred)
time_end = time.time()    #結束計時
time_c= time_end - time_start   #執行所花時間
print('time cost', time_c, 's')
print("f1_score:",f1_score(y_test,y_pred))
print("recall:",recall_score(y_test, y_pred))
print('precision: %.3f' % precision_score(y_test, y_pred))
print('accuracy: %.3f' % accuracy_score(y_test, y_pred))
learningcurve(history)

classifier.save("report_exec_times_model.h5")