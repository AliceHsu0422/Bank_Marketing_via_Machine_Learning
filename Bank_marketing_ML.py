# -*- coding: utf-8 -*-
"""final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MmJC8JtINZ1vvxTjy0-AUOOHjJttAwX_
"""

# Commented out IPython magic to ensure Python compatibility.
import time
import numpy as np
import pandas as pd

from sklearn.utils import shuffle
from sklearn.compose import make_column_selector as selector
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,OrdinalEncoder,RobustScaler,StandardScaler

from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression,BayesianRidge
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,StackingClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
import xgboost as xgb
from xgboost import XGBClassifier

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score
from sklearn.metrics import confusion_matrix

import optuna
from optuna import trial
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use('ggplot')

"""### 資料集提供者有說資料有按照日期排序，因此先將順序打亂"""

bank=pd.read_csv(r'bank-additional-full.csv',sep=";")

bank=shuffle(bank,random_state=42).reset_index(drop=True)

bank.head()

"""### 資料並無缺值"""

bank.info()

X=bank.drop('y',axis=1)
Y=bank['y'].map({'no':0,'yes':1})

"""### 先進行訓練、測試及驗證料切割，避免接下來資料前處理造成資訊洩漏
+ 訓練:80%
+ 測試:20%
"""

X_t,X_test,y_t,y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=42)

"""### 可以看到campaign、pdays、previous雖為數值資料，但從最大最小以及分位數值來看，可能有大量一樣的數值，因此進行離散化會比較好"""

X_t.describe()

"""### 在campaign中，3次內佔了81%"""

X_t['campaign'].value_counts(normalize=True)

sum(X_t['campaign'].value_counts(normalize=True)[0:3])

"""### 在pdays中，999占了96%左右"""

X_t['pdays'].value_counts(normalize=True)

"""### 在previous中，0和1佔了97%左右"""

X_t['previous'].value_counts(normalize=True)

"""### 因此將campaign、pdays、previous做以下處理轉換成類別變數:
+ campaign:分成1、2、3次以及3次以上(以4代表)
+ pdays:分成0:999；1:其他，999代表以前未聯繫客戶，1代表之前有聯繫過客戶
+ previous:分成0、1次和1次以上(以2代表)
"""

def num_to_cat(data):
    for feature in data:
        feature['campaign']=feature['campaign'].map(lambda x:4 if x>3 else x).astype(object)
        feature['pdays']=feature['pdays'].map(lambda x:0 if x==999 else 1).astype(object)
        feature['previous']=feature['previous'].map(lambda x:2 if x>1 else x).astype(object)

num_to_cat([X_t,X_test])

"""#### 我們關心的議題是y=1的標籤，也就是subscribed a term deposit的客戶
#### 從y_t可以看到，存在類別不平衡問題，若後續建模準確率沒超過88.7%，須調整模型超參數或是處理不平衡問題
#### 或是以F1 score、Recall、Precision來評價模型
"""

y_t.value_counts(normalize=True)

"""### 資料前處理
+ 數值資料:RobustScaler，避免離群值影響
+ 類別資料:OrdinalEncoder
"""

numerical_columns_selector = selector(dtype_exclude=object)
numerical_columns = numerical_columns_selector(X_t)
categorical_columns_selector = selector(dtype_include=object)
categorical_columns = categorical_columns_selector(X_t)

numerical_transformer=RobustScaler()
categorical_transformer1=OrdinalEncoder()
categorical_transformer2=OneHotEncoder()
preprocessor=ColumnTransformer(
    transformers=[
        ('num',numerical_transformer,numerical_columns),
        ('cat',categorical_transformer1,categorical_columns)
    ]
)
newcols=numerical_columns+categorical_columns
X_t=pd.DataFrame(preprocessor.fit_transform(X_t),columns=newcols)
X_test=pd.DataFrame(preprocessor.transform(X_test),columns=newcols)

X_t

X_t.info()

"""### 特徵選取:挑選16個特徵(80%)，將Gini importances最低的四個特徵刪掉(default、contact、previous和loan)"""

rf=RandomForestClassifier(n_jobs=-1)
rf.fit(X_t,y_t)
importances = rf.feature_importances_

plt.figure(figsize=(25,8))
importance_df=pd.DataFrame({'features':newcols,'importance':importances})
sns.barplot(x='features', y='importance',data=importance_df,order=importance_df.sort_values('importance').features)

X_t.drop(['default','contact','previous','loan'],axis=1,inplace=True)
X_test.drop(['default','contact','previous','loan'],axis=1,inplace=True)

'''
s_selection=time.time()
sfs = SequentialFeatureSelector(rf, n_features_to_select=16,direction='backward',n_jobs=-1)
sfs.fit(X_t,y_t)
e_selection=time.time()
print(f'Features process cost {round((e_selection-s_selection),2)} s')
removed=set(X_t.columns)-set(X_t.columns[sfs.get_support()])
removed
'''

"""### 建模"""

best_params=[]
X_train,X_valid,y_train,y_valid=train_test_split(X_t,y_t,test_size=0.1,stratify=y_t,random_state=42)

def rf_objective(trial):
    params={
        'n_estimators':trial.suggest_int('n_estimators',100,1500,50),
        'max_depth':trial.suggest_int('max_depth',1,20),
        'max_features':trial.suggest_categorical('max_features',[None,'auto','log2']),
        'min_samples_leaf':trial.suggest_int('min_samples_leaf',1,50),
        'min_samples_split':trial.suggest_int('min_samples_split',2,50),
        }
    clf=RandomForestClassifier(n_jobs=-1,random_state=42,**params)
    clf.fit(X_train,y_train)
    acc=accuracy_score(y_valid,clf.predict(X_valid))
    return acc

def svc_objective(trial):
    params={
        'kernel':trial.suggest_categorical('kernel',['linear','poly','rbf','sigmoid']),
        'C':trial.suggest_float('C',1e-4,1e4,log=True),
        'gamma':trial.suggest_float('gamma',1e-4,1e4,log=True),
        }
    clf=SVC(max_iter=1000,random_state=42,**params)
    clf.fit(X_train,y_train)
    acc=accuracy_score(y_valid,clf.predict(X_valid))
    return acc

def xgb_objective(trial):
    params={
        'max_depth': trial.suggest_int('max_depth', 6, 15),
        "subsample": trial.suggest_float("subsample", 0.2, 1.0),
        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, 100),
        'eta': trial.suggest_float("eta", 1e-8, 1.0, log=True),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),
        'gamma': trial.suggest_float("gamma", 1e-8, 1.0, log=True),
        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),
        'grow_policy': trial.suggest_categorical("grow_policy", ["depthwise", "lossguide"]),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.2, 1.0)
        }
    clf=xgb.XGBClassifier(objective="binary:logistic",**params)
    clf.fit(X_train,y_train,eval_set=[(X_valid,y_valid)],verbose=False)
    acc=accuracy_score(y_valid,clf.predict(X_valid))
    return acc

def tune_result(obj,clf_name,times):
    s=time.time()
    study=optuna.create_study(direction='maximize')
    study.optimize(obj,times)
    e=time.time()
    print(f'{clf_name}')
    print('=========================================')
    print(f'Hyperparameters process of costs: {round((e-s),2)} s')
    print(f'Hyperparameters of best trial are: {study.best_trial.params}')
    best_params.append(study.best_trial.params)

tune_result(xgb_objective,'XGBoost',50)

tune_result(rf_objective,'RandomForest',50)

tune_result(svc_objective,'SupportVector Machine',50)

#del best_params[2:4]

best_params

lr=LogisticRegression()
svc=SVC(max_iter=500,**best_params[2])
rf=RandomForestClassifier(**best_params[1])
xg=XGBClassifier(objective="binary:logistic",**best_params[0])

def result(trainx,trainy,clf):
    clf.fit(trainx,trainy)
    pred=clf.predict(X_test)
    acc=accuracy_score(y_test,pred)
    pre=precision_score(y_test,pred)
    rec=recall_score(y_test,pred)
    f1=f1_score(y_test,pred)
    metric_list=[acc,pre,rec,f1]
    return metric_list
    '''print(f'{clf_name}')
    print('=========================================')
    print(f'Accuracy: {round(acc*100,2)}%')
    print(f'Precision: {round(pre*100,2)}%')
    print(f'Recall: {round(rec*100,2)}%')
    print(f'F1 score: {round(f1,4)}')'''
compare_list=[result(X_t,y_t,lr),result(X_t,y_t,svc),result(X_t,y_t,rf),result(X_t,y_t,xg)]

columns=['Logistic','SVM','RandomForest','XGboost']
index=['Accuracy','Precision','Recall','F1 score']
result_compare=pd.DataFrame(compare_list).T
result_compare.columns=columns
result_compare.index=index

result_compare

